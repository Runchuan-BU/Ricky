{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l2K7f1ZwG5Z"
   },
   "source": [
    "# HW5: Transformer\n",
    "\n",
    "\n",
    "\n",
    "This assignment will introduce you to \n",
    "\n",
    "1. Understanding the structure of transformer. \n",
    "\n",
    "2. Building a GPT model step by step\n",
    "\n",
    "3. Train a GPT language model to write few sentences.\n",
    "\n",
    "You can run this assignment on Colab or SCC. You are encouraged to use GPU to make training faster, but BE AWARE the Google Colab GPU limit: you are limited to use it for less than 12 hours continuously, after that you may not be able to access it for a particular duration of time unless you purchase Colab pro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tMPpAqRpR2N"
   },
   "source": [
    "## Q1 Sequence to Sequence Modelling with nn.Transformer (20 points)\n",
    "\n",
    "You will implement a part of transformer. This question aims to let you to get familiar with the transformer architecture purposed in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). This question is modified from the original pytorch tutorial [here](https://pytorch.org/tutorials/beginner/transformer_tutorial.html?highlight=transformer), you can refer it when you fill out the code. The general architecture of trasnsformer is shown in the figure below:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" width=\"360em\">\n",
    "\n",
    "This question requires you to implement a sequence to sequence model by encoder, which is the left part of the figure. You will use integrated layers in pytorch.\n",
    "\n",
    "The transformer model has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention`) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like [`nn.TransformerEncoder `](<https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>)\n",
    "in this tutorial) can be easily adapted/composed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkeaGn8INY9k"
   },
   "source": [
    "### Q1.1 Define the model \n",
    "In this question, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "``nn.TransformerEncoderLayer`` . Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function. We will see how to implement the ``PositionalEncoding`` in the later question. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHI3LBIcgGVO"
   },
   "source": [
    "In the following model, we only train a encoder model, which is the left part of the figure. Then we concatenate a Linear model `self.decoder` to replace the right part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ai9dTxjUNS5-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    '''\n",
    "    This is a transformer encoder model, the input arguments are as follows:\n",
    "    args:\n",
    "    ntoken:  dimension of tokens\n",
    "    ninp: dimension of input embeddings\n",
    "    nhid: dimension of the hidden encoding between two layers of TransformerEncoderLayer\n",
    "    nlayers: number of TransformerEncoderLayer layers\n",
    "    nhead: the number of heads in the multiheadattention model\n",
    "    '''\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # PositionalEncoding will be implemented in next section.\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        '''\n",
    "        You can use torch.triu and masked_fill to get an upper triangle mask. \n",
    "        The upper right entries are -inf, down left entries including the diagonal are 0.\n",
    "        '''\n",
    "        mask= (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask= mask.float()\n",
    "        mask= mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        '''\n",
    "        Fill the forward function according to the diagram above.\n",
    "        In the embedding layers, we multiply those weights by square root of \n",
    "        self.ninp.\n",
    "        '''\n",
    "        src = self.pos_encoder(self.encoder(src) * math.sqrt(self.ninp))\n",
    "        output = self.decoder(self.transformer_encoder(src, src_mask))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-BxUnw6MZf5"
   },
   "source": [
    "### Q1.2 Positional Encoding\n",
    "#### Q1.2.1 Fill the code block\n",
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6A0pUKNMpQ84"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        P=torch.arange(max_len).unsqueeze(1)\n",
    "        e=torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        Pe=torch.zeros(max_len, d_model)\n",
    "        Pe[:, 0::2] = torch.sin(P * e)\n",
    "        Pe[:, 1::2] = torch.cos(P * e)\n",
    "        Pe = Pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', Pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        return self.dropout(x + self.pe[:x.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7mD6P7eCaKs"
   },
   "source": [
    "#### Q1.2.2 Why do we need this positional encoding in the transformer architectrue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNI5lKuvCmHu"
   },
   "source": [
    "We need positional encoding in the Transformer architecture because the Transformer uses self-attention mechanisms to process the input sequence. Self-attention allows the model to attend to different parts of the input sequence during processing, but it does not take into account the order or position of the tokens in the sequence.\n",
    "\n",
    "To overcome this limitation, we use positional encoding to inject information about the relative or absolute position of the tokens in the input sequence. By adding the positional encoding to the input embeddings, the Transformer can capture the order of the tokens and attend to them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ-XugehY_0y"
   },
   "source": [
    "## Q2 Transformer Block for GPT (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SucjfHGh_q5v"
   },
   "source": [
    "### Q 2.1 Multi-head self-attention\n",
    "#### Q 2.1.1 The first part is multi-head self-attention. In this layer, you will need to:\n",
    "- Apply linear projections to convert the feature vector at each token into separate vectors for the query, key, and value. The input and output size of linear projection are both `n_embd`\n",
    "- Apply attention, scaling the logits by $\\frac{1}{\\sqrt{d_{qkv}}}$.\n",
    "- Ensure proper masking, such that padding tokens are never attended to.\n",
    "- Perform attention `n_head` times in parallel, where the results are concatenated and then projected using a linear layer.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/332139525/figure/fig3/AS:743081083158528@1554175744311/a-The-Transformer-model-architecture-b-left-Scaled-Dot-Product-Attention.ppm\" width=\"360em\">\n",
    "\n",
    "You should include two types of dropout in your code (with probability set by the  `dropout` argument):\n",
    "- Dropout should be applied to the output of the attention layer (just prior to the residual connection, denoted by \"Add & Norm\" in the first figure)\n",
    "- Dropout should *also* be applied after the final projection.\n",
    "Notes:\n",
    "- Query, key, and value vectors should have shape `[batch_size, n_heads, sequence_len, d_qkv]`\n",
    "- Apply a mask to the scaled dot product of Q and K, before the Softmax function. Let the entry to be a small enough number where the entry of the causal mask is zero. You can use `torch.tril` or `torch.triu` to create a mask, usually we define the mask as a lower triangular matrix. Lower left (incude the diagonal) entries are ones, rest of entries are zeros.\n",
    "Then apply `tensor.masked_fill()` to the output of the scaled dot product of Q and K (It is also the input of softmax). Where the mask is zero, set the input to softmax to a negative number with very large magnitude.\n",
    "- Attention logits and probabilities should have shape `[batch_size, n_heads, sequence_len, sequence_len]`\n",
    "- Vaswani et al. define the output of the attention layer as concatenating the various heads and then multiplying by a matrix $W^O$. It's also possible to implement this is a sum without ever calling `torch.cat`: note that $\\text{Concat}(head_1, \\ldots, head_h)W^O = head_1 W^O_1 + \\ldots + head_h W^O_h$ where $W^O = \\begin{bmatrix} W^O_1\\\\ \\vdots\\\\ W^O_h\\end{bmatrix}$. You may define the `self.proj` this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-81DlPewvDRR"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    You can also use torch.nn.MultiheadAttention to validate your implementation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        H = self.n_head\n",
    "\n",
    "        # Calculate query, key, and value projections\n",
    "        keys = self.key(x).view(B, T, H, C // H).transpose(1, 2)\n",
    "        queries = self.query(x).view(B, T, H, C // H).transpose(1, 2)\n",
    "        values = self.value(x).view(B, T, H, C // H).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(C // H)\n",
    "\n",
    "        # Apply causal mask\n",
    "        mask = torch.triu(torch.ones_like(attn_scores), diagonal=1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Calculate attention probabilities\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.attn_drop(attn_probs)\n",
    "\n",
    "        # Compute attended values\n",
    "        attended_values = torch.matmul(attn_probs, values).transpose(1, 2).contiguous()\n",
    "        attended_values = attended_values.view(B, T, C)\n",
    "\n",
    "        # Apply residual connection and dropout\n",
    "        y = self.resid_drop(self.proj(attended_values))\n",
    "        return x + y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9rHSW_iV8Zl"
   },
   "source": [
    "#### Q 2.1.2 Why do we need to divide a scale of the dot product of Q and K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g64isWe2WXYW"
   },
   "source": [
    "The scale factor in the dot product of Q and K is used to prevent the dot product from becoming too large or too small. Recall that the dot product of two vectors measures their similarity in terms of direction, and it is computed by multiplying the magnitude of the two vectors and the cosine of the angle between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKJV23RLAKT3"
   },
   "source": [
    "### Q2.2 Transformer\n",
    "We will implement the transformer block, which is the blue box in the figure. You can use `nn.LayerNorm` layer to apply layer norm. We defined the feed forward layer as `self.mlp`.\n",
    "\n",
    "Notice that where to use the layer norm is a design choice, you can change to see how it affect the final results in the application of Question 3.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/358519951/figure/fig8/AS:1122134894092288@1644549215188/The-GPT-1-architecture-proposed-in-Radford-et-al-a-It-is-composed-of-12-stacked.ppm\" width=\"240em\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "I_L0P2zHAOVt"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" an Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = MultiHeadSelfAttention(n_embd, n_head, block_size, attn_pdrop)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(resid_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        ln=self.attn(self.ln1(x))\n",
    "        x=x+self.dropout(ln)\n",
    "        return x+self.dropout(self.mlp(self.ln2(x)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygPhx7D_Bsga"
   },
   "source": [
    "## Q3 GPT on Addition (30 points)\n",
    "In this question, we will train an GPT transformer to do addition. We first need to get the dataset and encode addition equation to a vocabulary by integers since we want to use GPT dealing with sequences of integers, and completing them according to patterns in the data. \n",
    "\n",
    "  The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
    "  encoding will simply be the n-digit first number, n-digit second number, and (n+1)-digit result, all simply concatenated together. Because each addition problem is so structured, there is no need to bother the model with encoding +, =, or other tokens. Each possible sequence has the same length, and simply contains the raw digits of the addition problem. As a few examples, the 2-digit problems:\n",
    "- 85 + 50 = 135 becomes the sequence `[8, 5, 5, 0, 1, 3, 5]`\n",
    "- 6 + 39 = 45 becomes the sequence `[0, 6, 3, 9, 0, 4, 5]`\n",
    "\n",
    "We will also only train GPT on the final (n+1)-digits because the first two n-digits are always assumed to be given. So when we give GPT an exam later, we will e.g. feed it the sequence `[0, 6, 3, 9]`, which encodes that we'd like to add 6 + 39, and hope that the model completes the integer sequence with `[0, 4, 5]` in 3 sequential steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VUGuOjDBGX3H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JwKTM2RgGQ2n"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Define the addition dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ndigit, split):\n",
    "        self.split = split # train/test\n",
    "        self.ndigit = ndigit\n",
    "        self.vocab_size = 10 # 10 possible digits 0..9\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
    "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(num)\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        nd = 10**self.ndigit\n",
    "        a = idx // nd\n",
    "        b = idx %  nd\n",
    "        c = a + b\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
    "        dix = [int(s) for s in render] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
    "        y[:self.ndigit*2-1] = -100 \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lru7Mb4fGV0h",
    "outputId": "133b1fab-c8dd-4a5e-9583-199a739f245d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataset for e.g. 2-digit addition\n",
    "ndigit = 2\n",
    "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
    "test_dataset = AdditionDataset(ndigit=ndigit, split='test')\n",
    "train_dataset[0] # sample a training instance just to see what one raw example looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uZgtgQlJNEN"
   },
   "source": [
    "### Q3.1 Define the GPT model \n",
    "Now, we start constructing the GPT model. As is shown in the figure, there are 12 transformer blocks concatenated together. In our model, we use `n_layer` to represent the number of blocks. In this question, you need to do the following:\n",
    "\n",
    "- Define the `n_layer` transformer blocks `self.blocks`\n",
    "- Fill out the forward function. Note that the positional embedding is not hard coded as the original transformer, it is learned during training.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/358519951/figure/fig8/AS:1122134894092288@1644549215188/The-GPT-1-architecture-proposed-in-Radford-et-al-a-It-is-composed-of-12-stacked.ppm\" width=\"240em\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uCozm4EEByR7"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a squence size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, n_head, block_size, n_layer, embd_pdrop=0.1, attn_pdrop=0.1,resid_pdrop=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
    "        self.drop = nn.Dropout(embd_pdrop)\n",
    "        # transformer\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        \n",
    "        \"\"\" \"\"\"\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size, attn_pdrop, resid_pdrop) for _ in range(n_layer)\n",
    "        ])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        You don't need to change this function. This is setting specific parameters for optimization.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        b, t = x.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "        \"\"\"YOUR CODE HERE\"\"\"\n",
    "        token_embeddings = self.tok_emb(x)\n",
    "        position_embeddings = self.pos_emb[:, :t, :]\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWEYZY9DF4y7"
   },
   "source": [
    "###Q3.2 Training the model\n",
    "\n",
    "##### You will train the GPT model. Fill out the code of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ca4alq_1GOM5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA7CPuribP_h"
   },
   "source": [
    "Setting some parameters for training. Initialize the GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tsl9tjHrGOvt"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 150\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "# initialize a baby GPT model\n",
    "model = GPT(vocab_size = train_dataset.vocab_size, n_embd=128, n_head=4, block_size =  train_dataset.block_size, n_layer=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sgav_e5boOD"
   },
   "source": [
    "You need to fill out training process. \n",
    "- Forward the model with current batch `x`, `y`;\n",
    "- Zero the grad before update;\n",
    "- Backward the loss and update the model parameter;\n",
    "- You might want to use `torch.nn.utils.clip_grad_norm_`. The parameter max_norm is `config.grad_norm_clip`;\n",
    "- You will run this getting a loss around 0.1 and accuracy on both train and test around 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMflK14DyGX-"
   },
   "source": [
    "#### 3.2.1 Why do we need to use `torch.nn.utils.clip_grad_norm_` in training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QQFRv4I9Vv1"
   },
   "source": [
    "During training, gradient values can grow large, which can cause problems in optimization, such as exploding gradients. To avoid this issue, gradient clipping is applied to rescale gradients to a maximum norm value. torch.nn.utils.clip_grad_norm_ is a utility function provided by PyTorch that clips the norm of gradients across all parameters in a model to a maximum value. This function is used to ensure that the norm of the gradients does not exceed a certain threshold, which can lead to numerical instability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIhTJaPsM7LD",
    "outputId": "b0245bc8-3107-4ecd-d33f-137141dea693"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 8: train loss 1.97857. lr 5.994512e-04: 100%|██████████| 9/9 [00:00<00:00, 34.82it/s]\n",
      "epoch 2 iter 8: train loss 1.78857. lr 5.977197e-04: 100%|██████████| 9/9 [00:00<00:00, 36.27it/s]\n",
      "epoch 3 iter 8: train loss 1.69308. lr 5.948114e-04: 100%|██████████| 9/9 [00:00<00:00, 39.84it/s]\n",
      "epoch 4 iter 8: train loss 1.57619. lr 5.907379e-04: 100%|██████████| 9/9 [00:00<00:00, 40.28it/s]\n",
      "epoch 5 iter 8: train loss 1.53862. lr 5.855153e-04: 100%|██████████| 9/9 [00:00<00:00, 39.65it/s]\n",
      "epoch 6 iter 8: train loss 1.46657. lr 5.791641e-04: 100%|██████████| 9/9 [00:00<00:00, 38.75it/s]\n",
      "epoch 7 iter 8: train loss 1.40720. lr 5.717095e-04: 100%|██████████| 9/9 [00:00<00:00, 15.94it/s]\n",
      "epoch 8 iter 8: train loss 1.37622. lr 5.631810e-04: 100%|██████████| 9/9 [00:00<00:00, 26.73it/s]\n",
      "epoch 9 iter 8: train loss 1.31371. lr 5.536122e-04: 100%|██████████| 9/9 [00:00<00:00, 34.34it/s]\n",
      "epoch 10 iter 8: train loss 1.26130. lr 5.430411e-04: 100%|██████████| 9/9 [00:00<00:00, 34.63it/s]\n",
      "epoch 11 iter 8: train loss 1.21599. lr 5.315093e-04: 100%|██████████| 9/9 [00:00<00:00, 34.96it/s]\n",
      "epoch 12 iter 8: train loss 1.17739. lr 5.190624e-04: 100%|██████████| 9/9 [00:00<00:00, 34.82it/s]\n",
      "epoch 13 iter 8: train loss 1.15299. lr 5.057497e-04: 100%|██████████| 9/9 [00:00<00:00, 35.17it/s]\n",
      "epoch 14 iter 8: train loss 1.12507. lr 4.916238e-04: 100%|██████████| 9/9 [00:00<00:00, 36.13it/s]\n",
      "epoch 15 iter 8: train loss 1.09555. lr 4.767405e-04: 100%|██████████| 9/9 [00:00<00:00, 33.87it/s]\n",
      "epoch 16 iter 8: train loss 1.04768. lr 4.611586e-04: 100%|██████████| 9/9 [00:00<00:00, 37.69it/s]\n",
      "epoch 17 iter 8: train loss 1.01870. lr 4.449397e-04: 100%|██████████| 9/9 [00:00<00:00, 35.28it/s]\n",
      "epoch 18 iter 8: train loss 0.96838. lr 4.281479e-04: 100%|██████████| 9/9 [00:00<00:00, 38.86it/s]\n",
      "epoch 19 iter 8: train loss 0.96346. lr 4.108497e-04: 100%|██████████| 9/9 [00:00<00:00, 38.01it/s]\n",
      "epoch 20 iter 8: train loss 0.89466. lr 3.931133e-04: 100%|██████████| 9/9 [00:00<00:00, 40.67it/s]\n",
      "epoch 21 iter 8: train loss 0.85482. lr 3.750088e-04: 100%|██████████| 9/9 [00:00<00:00, 46.29it/s]\n",
      "epoch 22 iter 8: train loss 0.83714. lr 3.566079e-04: 100%|██████████| 9/9 [00:00<00:00, 42.31it/s]\n",
      "epoch 23 iter 8: train loss 0.80504. lr 3.379832e-04: 100%|██████████| 9/9 [00:00<00:00, 38.88it/s]\n",
      "epoch 24 iter 8: train loss 0.77280. lr 3.192084e-04: 100%|██████████| 9/9 [00:00<00:00, 40.03it/s]\n",
      "epoch 25 iter 8: train loss 0.75491. lr 3.003577e-04: 100%|██████████| 9/9 [00:00<00:00, 38.39it/s]\n",
      "epoch 26 iter 8: train loss 0.72464. lr 2.815056e-04: 100%|██████████| 9/9 [00:00<00:00, 44.80it/s]\n",
      "epoch 27 iter 8: train loss 0.70055. lr 2.627266e-04: 100%|██████████| 9/9 [00:00<00:00, 38.10it/s]\n",
      "epoch 28 iter 8: train loss 0.68158. lr 2.440948e-04: 100%|██████████| 9/9 [00:00<00:00, 42.35it/s]\n",
      "epoch 29 iter 8: train loss 0.66073. lr 2.256841e-04: 100%|██████████| 9/9 [00:00<00:00, 41.21it/s]\n",
      "epoch 30 iter 8: train loss 0.66022. lr 2.075671e-04: 100%|██████████| 9/9 [00:00<00:00, 39.62it/s]\n",
      "epoch 31 iter 8: train loss 0.63167. lr 1.898155e-04: 100%|██████████| 9/9 [00:00<00:00, 42.19it/s]\n",
      "epoch 32 iter 8: train loss 0.62696. lr 1.724993e-04: 100%|██████████| 9/9 [00:00<00:00, 37.48it/s]\n",
      "epoch 33 iter 8: train loss 0.61806. lr 1.556871e-04: 100%|██████████| 9/9 [00:00<00:00, 35.60it/s]\n",
      "epoch 34 iter 8: train loss 0.61543. lr 1.394453e-04: 100%|██████████| 9/9 [00:00<00:00, 34.17it/s]\n",
      "epoch 35 iter 8: train loss 0.58259. lr 1.238381e-04: 100%|██████████| 9/9 [00:00<00:00, 37.44it/s]\n",
      "epoch 36 iter 8: train loss 0.59357. lr 1.089272e-04: 100%|██████████| 9/9 [00:00<00:00, 30.73it/s]\n",
      "epoch 37 iter 8: train loss 0.58115. lr 9.477150e-05: 100%|██████████| 9/9 [00:00<00:00, 36.92it/s]\n",
      "epoch 38 iter 8: train loss 0.57544. lr 8.142699e-05: 100%|██████████| 9/9 [00:00<00:00, 33.62it/s]\n",
      "epoch 39 iter 8: train loss 0.56984. lr 6.894639e-05: 100%|██████████| 9/9 [00:00<00:00, 31.95it/s]\n",
      "epoch 40 iter 8: train loss 0.57091. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 31.33it/s]\n",
      "epoch 41 iter 8: train loss 0.55532. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 33.21it/s]\n",
      "epoch 42 iter 8: train loss 0.56477. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 35.83it/s]\n",
      "epoch 43 iter 8: train loss 0.56639. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 34.49it/s]\n",
      "epoch 44 iter 8: train loss 0.55302. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 33.23it/s]\n",
      "epoch 45 iter 8: train loss 0.57445. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 31.70it/s]\n",
      "epoch 46 iter 8: train loss 0.57440. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 39.67it/s]\n",
      "epoch 47 iter 8: train loss 0.55323. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 37.07it/s]\n",
      "epoch 48 iter 8: train loss 0.55545. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 37.14it/s]\n",
      "epoch 49 iter 8: train loss 0.53764. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 39.73it/s]\n",
      "epoch 50 iter 8: train loss 0.54631. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 34.91it/s]\n",
      "epoch 51 iter 8: train loss 0.53656. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 33.72it/s]\n",
      "epoch 52 iter 8: train loss 0.53036. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 38.32it/s]\n",
      "epoch 53 iter 8: train loss 0.52868. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 36.42it/s]\n",
      "epoch 54 iter 8: train loss 0.53232. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 36.21it/s]\n",
      "epoch 55 iter 8: train loss 0.52391. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 39.37it/s]\n",
      "epoch 56 iter 8: train loss 0.52273. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 36.16it/s]\n",
      "epoch 57 iter 8: train loss 0.49706. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 36.70it/s]\n",
      "epoch 58 iter 8: train loss 0.50823. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 38.11it/s]\n",
      "epoch 59 iter 8: train loss 0.49910. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 36.82it/s]\n",
      "epoch 60 iter 8: train loss 0.51292. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 37.65it/s]\n",
      "epoch 61 iter 8: train loss 0.50973. lr 6.894639e-05: 100%|██████████| 9/9 [00:00<00:00, 37.54it/s]\n",
      "epoch 62 iter 8: train loss 0.50519. lr 8.142699e-05: 100%|██████████| 9/9 [00:00<00:00, 40.13it/s]\n",
      "epoch 63 iter 8: train loss 0.49855. lr 9.477150e-05: 100%|██████████| 9/9 [00:00<00:00, 37.17it/s]\n",
      "epoch 64 iter 8: train loss 0.49919. lr 1.089272e-04: 100%|██████████| 9/9 [00:00<00:00, 37.46it/s]\n",
      "epoch 65 iter 8: train loss 0.47776. lr 1.238381e-04: 100%|██████████| 9/9 [00:00<00:00, 39.85it/s]\n",
      "epoch 66 iter 8: train loss 0.50379. lr 1.394453e-04: 100%|██████████| 9/9 [00:00<00:00, 42.27it/s]\n",
      "epoch 67 iter 8: train loss 0.46948. lr 1.556871e-04: 100%|██████████| 9/9 [00:00<00:00, 37.94it/s]\n",
      "epoch 68 iter 8: train loss 0.47564. lr 1.724993e-04: 100%|██████████| 9/9 [00:00<00:00, 38.61it/s]\n",
      "epoch 69 iter 8: train loss 0.44084. lr 1.898155e-04: 100%|██████████| 9/9 [00:00<00:00, 34.27it/s]\n",
      "epoch 70 iter 8: train loss 0.44333. lr 2.075671e-04: 100%|██████████| 9/9 [00:00<00:00, 35.67it/s]\n",
      "epoch 71 iter 8: train loss 0.45146. lr 2.256841e-04: 100%|██████████| 9/9 [00:00<00:00, 35.73it/s]\n",
      "epoch 72 iter 8: train loss 0.42352. lr 2.440948e-04: 100%|██████████| 9/9 [00:00<00:00, 37.16it/s]\n",
      "epoch 73 iter 8: train loss 0.42834. lr 2.627266e-04: 100%|██████████| 9/9 [00:00<00:00, 39.28it/s]\n",
      "epoch 74 iter 8: train loss 0.43804. lr 2.815056e-04: 100%|██████████| 9/9 [00:00<00:00, 39.85it/s]\n",
      "epoch 75 iter 8: train loss 0.41736. lr 3.003577e-04: 100%|██████████| 9/9 [00:00<00:00, 37.19it/s]\n",
      "epoch 76 iter 8: train loss 0.40269. lr 3.192084e-04: 100%|██████████| 9/9 [00:00<00:00, 35.52it/s]\n",
      "epoch 77 iter 8: train loss 0.38359. lr 3.379832e-04: 100%|██████████| 9/9 [00:00<00:00, 33.55it/s]\n",
      "epoch 78 iter 8: train loss 0.39841. lr 3.566079e-04: 100%|██████████| 9/9 [00:00<00:00, 40.35it/s]\n",
      "epoch 79 iter 8: train loss 0.40016. lr 3.750088e-04: 100%|██████████| 9/9 [00:00<00:00, 36.55it/s]\n",
      "epoch 80 iter 8: train loss 0.37626. lr 3.931133e-04: 100%|██████████| 9/9 [00:00<00:00, 37.33it/s]\n",
      "epoch 81 iter 8: train loss 0.35066. lr 4.108497e-04: 100%|██████████| 9/9 [00:00<00:00, 36.97it/s]\n",
      "epoch 82 iter 8: train loss 0.36700. lr 4.281479e-04: 100%|██████████| 9/9 [00:00<00:00, 36.44it/s]\n",
      "epoch 83 iter 8: train loss 0.35289. lr 4.449397e-04: 100%|██████████| 9/9 [00:00<00:00, 39.32it/s]\n",
      "epoch 84 iter 8: train loss 0.33962. lr 4.611586e-04: 100%|██████████| 9/9 [00:00<00:00, 35.83it/s]\n",
      "epoch 85 iter 8: train loss 0.31218. lr 4.767405e-04: 100%|██████████| 9/9 [00:00<00:00, 36.98it/s]\n",
      "epoch 86 iter 8: train loss 0.32064. lr 4.916238e-04: 100%|██████████| 9/9 [00:00<00:00, 43.14it/s]\n",
      "epoch 87 iter 8: train loss 0.32611. lr 5.057497e-04: 100%|██████████| 9/9 [00:00<00:00, 37.22it/s]\n",
      "epoch 88 iter 8: train loss 0.32251. lr 5.190624e-04: 100%|██████████| 9/9 [00:00<00:00, 39.54it/s]\n",
      "epoch 89 iter 8: train loss 0.29103. lr 5.315093e-04: 100%|██████████| 9/9 [00:00<00:00, 34.82it/s]\n",
      "epoch 90 iter 8: train loss 0.29444. lr 5.430411e-04: 100%|██████████| 9/9 [00:00<00:00, 35.83it/s]\n",
      "epoch 91 iter 8: train loss 0.28212. lr 5.536122e-04: 100%|██████████| 9/9 [00:00<00:00, 35.64it/s]\n",
      "epoch 92 iter 8: train loss 0.28498. lr 5.631810e-04: 100%|██████████| 9/9 [00:00<00:00, 37.78it/s]\n",
      "epoch 93 iter 8: train loss 0.27181. lr 5.717095e-04: 100%|██████████| 9/9 [00:00<00:00, 38.46it/s]\n",
      "epoch 94 iter 8: train loss 0.27035. lr 5.791641e-04: 100%|██████████| 9/9 [00:00<00:00, 38.86it/s]\n",
      "epoch 95 iter 8: train loss 0.25464. lr 5.855153e-04: 100%|██████████| 9/9 [00:00<00:00, 36.15it/s]\n",
      "epoch 96 iter 8: train loss 0.25147. lr 5.907379e-04: 100%|██████████| 9/9 [00:00<00:00, 36.25it/s]\n",
      "epoch 97 iter 8: train loss 0.23259. lr 5.948114e-04: 100%|██████████| 9/9 [00:00<00:00, 39.88it/s]\n",
      "epoch 98 iter 8: train loss 0.23195. lr 5.977197e-04: 100%|██████████| 9/9 [00:00<00:00, 37.58it/s]\n",
      "epoch 99 iter 8: train loss 0.23300. lr 5.994512e-04: 100%|██████████| 9/9 [00:00<00:00, 38.78it/s]\n",
      "epoch 100 iter 8: train loss 0.22393. lr 5.999991e-04: 100%|██████████| 9/9 [00:00<00:00, 38.64it/s]\n",
      "epoch 101 iter 8: train loss 0.20723. lr 5.993613e-04: 100%|██████████| 9/9 [00:00<00:00, 37.59it/s]\n",
      "epoch 102 iter 8: train loss 0.20482. lr 5.975402e-04: 100%|██████████| 9/9 [00:00<00:00, 36.95it/s]\n",
      "epoch 103 iter 8: train loss 0.20461. lr 5.945431e-04: 100%|██████████| 9/9 [00:00<00:00, 37.86it/s]\n",
      "epoch 104 iter 8: train loss 0.20306. lr 5.903818e-04: 100%|██████████| 9/9 [00:00<00:00, 37.46it/s]\n",
      "epoch 105 iter 8: train loss 0.18018. lr 5.850728e-04: 100%|██████████| 9/9 [00:00<00:00, 41.42it/s]\n",
      "epoch 106 iter 8: train loss 0.21999. lr 5.786370e-04: 100%|██████████| 9/9 [00:00<00:00, 37.09it/s]\n",
      "epoch 107 iter 8: train loss 0.17961. lr 5.710999e-04: 100%|██████████| 9/9 [00:00<00:00, 36.29it/s]\n",
      "epoch 108 iter 8: train loss 0.20602. lr 5.624912e-04: 100%|██████████| 9/9 [00:00<00:00, 37.23it/s]\n",
      "epoch 109 iter 8: train loss 0.18751. lr 5.528450e-04: 100%|██████████| 9/9 [00:00<00:00, 39.34it/s]\n",
      "epoch 110 iter 8: train loss 0.16927. lr 5.421995e-04: 100%|██████████| 9/9 [00:00<00:00, 40.23it/s]\n",
      "epoch 111 iter 8: train loss 0.18397. lr 5.305966e-04: 100%|██████████| 9/9 [00:00<00:00, 42.40it/s]\n",
      "epoch 112 iter 8: train loss 0.16173. lr 5.180823e-04: 100%|██████████| 9/9 [00:00<00:00, 36.35it/s]\n",
      "epoch 113 iter 8: train loss 0.15452. lr 5.047061e-04: 100%|██████████| 9/9 [00:00<00:00, 35.65it/s]\n",
      "epoch 114 iter 8: train loss 0.16084. lr 4.905207e-04: 100%|██████████| 9/9 [00:00<00:00, 38.06it/s]\n",
      "epoch 115 iter 8: train loss 0.14600. lr 4.755823e-04: 100%|██████████| 9/9 [00:00<00:00, 34.58it/s]\n",
      "epoch 116 iter 8: train loss 0.14134. lr 4.599499e-04: 100%|██████████| 9/9 [00:00<00:00, 38.04it/s]\n",
      "epoch 117 iter 8: train loss 0.15542. lr 4.436853e-04: 100%|██████████| 9/9 [00:00<00:00, 36.84it/s]\n",
      "epoch 118 iter 8: train loss 0.15012. lr 4.268527e-04: 100%|██████████| 9/9 [00:00<00:00, 35.69it/s]\n",
      "epoch 119 iter 8: train loss 0.14952. lr 4.095188e-04: 100%|██████████| 9/9 [00:00<00:00, 36.77it/s]\n",
      "epoch 120 iter 8: train loss 0.16004. lr 3.917520e-04: 100%|██████████| 9/9 [00:00<00:00, 41.18it/s]\n",
      "epoch 121 iter 8: train loss 0.13807. lr 3.736225e-04: 100%|██████████| 9/9 [00:00<00:00, 38.56it/s]\n",
      "epoch 122 iter 8: train loss 0.14228. lr 3.552021e-04: 100%|██████████| 9/9 [00:00<00:00, 37.16it/s]\n",
      "epoch 123 iter 8: train loss 0.13675. lr 3.365635e-04: 100%|██████████| 9/9 [00:00<00:00, 38.82it/s]\n",
      "epoch 124 iter 8: train loss 0.13111. lr 3.177803e-04: 100%|██████████| 9/9 [00:00<00:00, 38.69it/s]\n",
      "epoch 125 iter 8: train loss 0.13478. lr 2.989269e-04: 100%|██████████| 9/9 [00:00<00:00, 35.86it/s]\n",
      "epoch 126 iter 8: train loss 0.14524. lr 2.800777e-04: 100%|██████████| 9/9 [00:00<00:00, 36.24it/s]\n",
      "epoch 127 iter 8: train loss 0.13338. lr 2.613072e-04: 100%|██████████| 9/9 [00:00<00:00, 36.06it/s]\n",
      "epoch 128 iter 8: train loss 0.11796. lr 2.426897e-04: 100%|██████████| 9/9 [00:00<00:00, 38.33it/s]\n",
      "epoch 129 iter 8: train loss 0.13491. lr 2.242987e-04: 100%|██████████| 9/9 [00:00<00:00, 38.63it/s]\n",
      "epoch 130 iter 8: train loss 0.12139. lr 2.062069e-04: 100%|██████████| 9/9 [00:00<00:00, 35.92it/s]\n",
      "epoch 131 iter 8: train loss 0.11174. lr 1.884859e-04: 100%|██████████| 9/9 [00:00<00:00, 38.58it/s]\n",
      "epoch 132 iter 8: train loss 0.11751. lr 1.712056e-04: 100%|██████████| 9/9 [00:00<00:00, 37.19it/s]\n",
      "epoch 133 iter 8: train loss 0.12710. lr 1.544343e-04: 100%|██████████| 9/9 [00:00<00:00, 37.16it/s]\n",
      "epoch 134 iter 8: train loss 0.13642. lr 1.382384e-04: 100%|██████████| 9/9 [00:00<00:00, 36.39it/s]\n",
      "epoch 135 iter 8: train loss 0.12863. lr 1.226819e-04: 100%|██████████| 9/9 [00:00<00:00, 36.35it/s]\n",
      "epoch 136 iter 8: train loss 0.13185. lr 1.078263e-04: 100%|██████████| 9/9 [00:00<00:00, 36.91it/s]\n",
      "epoch 137 iter 8: train loss 0.10309. lr 9.373018e-05: 100%|██████████| 9/9 [00:00<00:00, 36.04it/s]\n",
      "epoch 138 iter 8: train loss 0.11949. lr 8.044939e-05: 100%|██████████| 9/9 [00:00<00:00, 40.55it/s]\n",
      "epoch 139 iter 8: train loss 0.10860. lr 6.803638e-05: 100%|██████████| 9/9 [00:00<00:00, 37.67it/s]\n",
      "epoch 140 iter 8: train loss 0.09692. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 41.43it/s]\n",
      "epoch 141 iter 8: train loss 0.10064. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 39.08it/s]\n",
      "epoch 142 iter 8: train loss 0.11903. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 41.99it/s]\n",
      "epoch 143 iter 8: train loss 0.10886. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 40.10it/s]\n",
      "epoch 144 iter 8: train loss 0.11105. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 35.91it/s]\n",
      "epoch 145 iter 8: train loss 0.11179. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 39.04it/s]\n",
      "epoch 146 iter 8: train loss 0.10545. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 36.23it/s]\n",
      "epoch 147 iter 8: train loss 0.11057. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 37.25it/s]\n",
      "epoch 148 iter 8: train loss 0.10767. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 37.62it/s]\n",
      "epoch 149 iter 8: train loss 0.10636. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 42.00it/s]\n",
      "epoch 150 iter 8: train loss 0.10540. lr 6.000000e-05: 100%|██████████| 9/9 [00:00<00:00, 37.15it/s]\n"
     ]
    }
   ],
   "source": [
    "config = TrainerConfig(max_epochs=150, batch_size=1024, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
    "                      num_workers=4)\n",
    "\n",
    "device = 'gpu'\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.cuda.current_device()\n",
    "  model.to(device)\n",
    "optimizer = model.configure_optimizers(config)\n",
    "\n",
    "\n",
    "\n",
    "tokens = 0\n",
    "for epoch in range(config.max_epochs):\n",
    "    model.train()\n",
    "    data = train_dataset \n",
    "    loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                        batch_size=config.batch_size,\n",
    "                        num_workers=config.num_workers)\n",
    "    losses = []\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader)) \n",
    "    for iter, (x, y) in pbar:\n",
    "        # place data on the correct device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # forward the model\n",
    "        \"\"\" CODE HERE \"\"\"\n",
    "        _, loss = model(x, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        \"\"\" CODE HERE END \"\"\"\n",
    "\n",
    "\n",
    "        # decay the learning rate based on our progress\n",
    "        if config.lr_decay:\n",
    "            tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "            if tokens < config.warmup_tokens:\n",
    "                # linear warmup\n",
    "                lr_mult = float(tokens) / float(max(1, config.warmup_tokens))\n",
    "            else:\n",
    "                # cosine learning rate decay\n",
    "                progress = float(tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "            lr = config.learning_rate * lr_mult\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            lr = config.learning_rate\n",
    "        # report progress\n",
    "        pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}. lr {lr:e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0gIIlMJiSFC"
   },
   "source": [
    "Now you can run the following code to test the training data sne testing data. You should reach more than 95% correctness on both train and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-gARh2KE3K5A"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. \n",
    "    \"\"\"\n",
    "    block_size = train_dataset.block_size\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits = model(x_cond)  # Change this line\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "    return x\n",
    "def Addition_GPT(dataset, batch_size=32, max_batches=-1):\n",
    "    \n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        d1d2 = x[:, :ndigit*2]\n",
    "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
    "        d3 = d1d2d3[:, -(ndigit+1):]\n",
    "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(device)\n",
    "        # decode the integers from individual digits\n",
    "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
    "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
    "        d3i_pred = (d3 * factors).sum(1)\n",
    "        d3i_gt = d1i + d2i\n",
    "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            judge = 'CORRECT' if correct[i] else 'WRONG'\n",
    "            if not correct[i]:\n",
    "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
    "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbHtf3tqjJAa",
    "outputId": "13cd6f9c-d212-4eb8-dc15-52fd38dd4c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 9000/9000 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "Addition_GPT(train_dataset, batch_size=1024, max_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_sljQGijJ0n",
    "outputId": "c3df64af-4277-4cc8-c856-f50651d203a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final score: 1000/1000 = 100.00% correct\n"
     ]
    }
   ],
   "source": [
    "Addition_GPT(test_dataset, batch_size=1024, max_batches=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_wZdbFf842s"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Q4 minGPT Text Completion(20 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INE5JhJN9c_P"
   },
   "source": [
    "In this question, we will train a GPT to write something fun. First, we need to download some Harry Potter novels as the training dataset. Then a `CharDataset` class is provided to help you prepare training data in the format of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qy7rPy99cPX",
    "outputId": "d73e78c7-c699-47c6-e1df-7c039f91c2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-25 22:24:54--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 439742 (429K) [text/plain]\n",
      "Saving to: 'J. K. Rowling - Harry Potter 1 - Sorcerer\\'s Stone.txt.1'\n",
      "\n",
      "100%[======================================>] 439,742     --.-K/s   in 0.04s   \n",
      "\n",
      "2023-04-25 22:24:54 (9.92 MB/s) - 'J. K. Rowling - Harry Potter 1 - Sorcerer\\'s Stone.txt.1' saved [439742/439742]\n",
      "\n",
      "--2023-04-25 22:24:55--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 492130 (481K) [text/plain]\n",
      "Saving to: 'J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt.1'\n",
      "\n",
      "100%[======================================>] 492,130     --.-K/s   in 0.03s   \n",
      "\n",
      "2023-04-25 22:24:55 (16.0 MB/s) - 'J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt.1' saved [492130/492130]\n",
      "\n",
      "--2023-04-25 22:24:55--  https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 611601 (597K) [text/plain]\n",
      "Saving to: 'J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt.1'\n",
      "\n",
      "100%[======================================>] 611,601     --.-K/s   in 0.03s   \n",
      "\n",
      "2023-04-25 22:24:55 (17.5 MB/s) - 'J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt.1' saved [611601/611601]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
    "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%202%20-%20The%20Chamber%20Of%20Secrets.txt\"\n",
    "!wget \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%203%20-%20Prisoner%20of%20Azkaban.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eBlbtywRkH8v"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from collections.abc import Iterable\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "klywauhX92uE"
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / (self.block_size + 1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # we're actually going to \"cheat\" and pick a spot in the dataset at random\n",
    "        i = np.random.randint(0, len(self.data) - (self.block_size + 1))\n",
    "        chunk = self.data[i:i+self.block_size+1]\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEM5HuNGMXMp",
    "outputId": "02d0bcfd-a996-4ef6-ead1-bb16716c52cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1543473 characters, 87 unique.\n"
     ]
    }
   ],
   "source": [
    "# the \"block size\" is the number of characters the model takes as input.\n",
    "# in this case, it can look at up to 128 characters when predicting the next\n",
    "# character.\n",
    "block_size = 128 # spatial extent of the model for its context\n",
    "\n",
    "# For our training set, we will use the text of the first three Harry Potter books.\n",
    "text = open(\"J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt\", 'rb').read()\n",
    "text += open('J. K. Rowling - Harry Potter 2 - The Chamber Of Secrets.txt', 'rb').read()\n",
    "text += open('J. K. Rowling - Harry Potter 3 - Prisoner of Azkaban.txt', 'rb').read()\n",
    "\n",
    "train_dataset = CharDataset(text, block_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpLa2b2V9na5"
   },
   "source": [
    "### Q4.1 Train the model on texts and Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaB_bFtBOgJO"
   },
   "source": [
    "Here we are going to train a language model on the `train_dataset`. In this question, you need to finish writing a training loop and save the best model (either the loss is small or the model output is fun) you get. You can refer to the training loop from Q3, but remember to save the model checkpoint(s) while training or at the end of the training loop - You will need it in the next question! \n",
    "\n",
    "\n",
    "\n",
    "In the next cell, an example setup and parameters for the minGPT language model is provided, feel free to change the parameters. If you encounter a training loss plateau - loss is oscillating around some number and the tendency of decreasing is not obvious - and cannot overcome it when trying different parameter combinations, \n",
    "- document the parameters in `TrainerConfig`, training loss curves/loss from the best model you have , the machine you used to train the model and \n",
    "- propose a solution to overcome this plateau  \n",
    "\n",
    "If you there is no such plateau, tell us what are the parameters in the `TrainConfig` and which GPU is used for training. \n",
    "\n",
    "In this question, you are requried to \n",
    "- write a training loop that saves some checkpoints in the training loop\n",
    "- show some discussion on the training loss plateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "903fz6PlmuWv"
   },
   "outputs": [],
   "source": [
    "config = TrainerConfig(max_epochs=50, batch_size=64, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=150*len(train_dataset),\n",
    "                      num_workers=4)\n",
    "\n",
    "model = GPT(vocab_size=train_dataset.vocab_size,n_embd=128, n_head=8,block_size=train_dataset.block_size,n_layer=12,)\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.cuda.current_device()\n",
    "  model.to(device)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uZZU2KW2Fh1N"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-A9oe3XF82d",
    "outputId": "5db76708-fd5e-4fd5-98c6-2caa25432def"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 186: train loss 1.11559: 100%|██████████| 187/187 [00:12<00:00, 15.20it/s]\n",
      "epoch 2 iter 186: train loss 1.11478: 100%|██████████| 187/187 [00:12<00:00, 15.06it/s]\n",
      "epoch 3 iter 186: train loss 1.12161: 100%|██████████| 187/187 [00:12<00:00, 15.06it/s]\n",
      "epoch 4 iter 186: train loss 1.08764: 100%|██████████| 187/187 [00:12<00:00, 15.51it/s]\n",
      "epoch 5 iter 186: train loss 1.08733: 100%|██████████| 187/187 [00:11<00:00, 15.91it/s]\n",
      "epoch 6 iter 186: train loss 1.11215: 100%|██████████| 187/187 [00:12<00:00, 15.49it/s]\n",
      "epoch 7 iter 186: train loss 1.09134: 100%|██████████| 187/187 [00:11<00:00, 16.01it/s]\n",
      "epoch 8 iter 186: train loss 1.13979: 100%|██████████| 187/187 [00:12<00:00, 15.35it/s]\n",
      "epoch 9 iter 186: train loss 1.10191: 100%|██████████| 187/187 [00:12<00:00, 15.13it/s]\n",
      "epoch 10 iter 186: train loss 1.09025: 100%|██████████| 187/187 [00:12<00:00, 15.27it/s]\n",
      "epoch 11 iter 186: train loss 1.09257: 100%|██████████| 187/187 [00:12<00:00, 15.32it/s]\n",
      "epoch 12 iter 186: train loss 1.07961: 100%|██████████| 187/187 [00:12<00:00, 15.33it/s]\n",
      "epoch 13 iter 186: train loss 1.08217: 100%|██████████| 187/187 [00:12<00:00, 15.31it/s]\n",
      "epoch 14 iter 186: train loss 1.07356: 100%|██████████| 187/187 [00:12<00:00, 15.33it/s]\n",
      "epoch 15 iter 186: train loss 1.07609: 100%|██████████| 187/187 [00:11<00:00, 15.86it/s]\n",
      "epoch 16 iter 186: train loss 1.05696: 100%|██████████| 187/187 [00:11<00:00, 15.75it/s]\n",
      "epoch 17 iter 186: train loss 1.08073: 100%|██████████| 187/187 [00:12<00:00, 15.55it/s]\n",
      "epoch 18 iter 186: train loss 1.11332: 100%|██████████| 187/187 [00:11<00:00, 15.82it/s]\n",
      "epoch 19 iter 186: train loss 1.12417: 100%|██████████| 187/187 [00:12<00:00, 15.17it/s]\n",
      "epoch 20 iter 186: train loss 1.08538: 100%|██████████| 187/187 [00:12<00:00, 14.79it/s]\n",
      "epoch 21 iter 186: train loss 1.09008: 100%|██████████| 187/187 [00:12<00:00, 15.27it/s]\n",
      "epoch 22 iter 186: train loss 1.10534: 100%|██████████| 187/187 [00:12<00:00, 15.23it/s]\n",
      "epoch 23 iter 186: train loss 1.05717: 100%|██████████| 187/187 [00:12<00:00, 15.09it/s]\n",
      "epoch 24 iter 186: train loss 1.05693: 100%|██████████| 187/187 [00:12<00:00, 15.27it/s]\n",
      "epoch 25 iter 186: train loss 1.05544: 100%|██████████| 187/187 [00:12<00:00, 15.38it/s]\n",
      "epoch 26 iter 186: train loss 1.06235: 100%|██████████| 187/187 [00:11<00:00, 15.99it/s]\n",
      "epoch 27 iter 186: train loss 1.07879: 100%|██████████| 187/187 [00:11<00:00, 16.00it/s]\n",
      "epoch 28 iter 186: train loss 1.08145: 100%|██████████| 187/187 [00:11<00:00, 15.91it/s]\n",
      "epoch 29 iter 186: train loss 1.06809: 100%|██████████| 187/187 [00:12<00:00, 15.50it/s]\n",
      "epoch 30 iter 186: train loss 1.04468: 100%|██████████| 187/187 [00:12<00:00, 15.32it/s]\n",
      "epoch 31 iter 186: train loss 1.04690: 100%|██████████| 187/187 [00:12<00:00, 15.23it/s]\n",
      "epoch 32 iter 186: train loss 1.05503: 100%|██████████| 187/187 [00:12<00:00, 15.18it/s]\n",
      "epoch 33 iter 186: train loss 1.03848: 100%|██████████| 187/187 [00:12<00:00, 15.33it/s]\n",
      "epoch 34 iter 186: train loss 1.05277: 100%|██████████| 187/187 [00:12<00:00, 15.34it/s]\n",
      "epoch 35 iter 186: train loss 1.05576: 100%|██████████| 187/187 [00:12<00:00, 15.32it/s]\n",
      "epoch 36 iter 186: train loss 1.02624: 100%|██████████| 187/187 [00:12<00:00, 15.48it/s]\n",
      "epoch 37 iter 186: train loss 1.03910: 100%|██████████| 187/187 [00:11<00:00, 16.00it/s]\n",
      "epoch 38 iter 186: train loss 1.04450: 100%|██████████| 187/187 [00:11<00:00, 15.71it/s]\n",
      "epoch 39 iter 186: train loss 1.06070: 100%|██████████| 187/187 [00:11<00:00, 15.69it/s]\n",
      "epoch 40 iter 186: train loss 1.02770: 100%|██████████| 187/187 [00:12<00:00, 15.26it/s]\n",
      "epoch 41 iter 186: train loss 1.05664: 100%|██████████| 187/187 [00:12<00:00, 14.97it/s]\n",
      "epoch 42 iter 186: train loss 1.06075: 100%|██████████| 187/187 [00:12<00:00, 15.16it/s]\n",
      "epoch 43 iter 186: train loss 1.05709: 100%|██████████| 187/187 [00:12<00:00, 15.31it/s]\n",
      "epoch 44 iter 186: train loss 1.05905: 100%|██████████| 187/187 [00:12<00:00, 15.31it/s]\n",
      "epoch 45 iter 186: train loss 1.05798: 100%|██████████| 187/187 [00:12<00:00, 15.29it/s]\n",
      "epoch 46 iter 186: train loss 1.05722: 100%|██████████| 187/187 [00:12<00:00, 15.32it/s]\n",
      "epoch 47 iter 186: train loss 1.03239: 100%|██████████| 187/187 [00:12<00:00, 15.55it/s]\n",
      "epoch 48 iter 186: train loss 1.01950: 100%|██████████| 187/187 [00:11<00:00, 15.98it/s]\n",
      "epoch 49 iter 186: train loss 1.02147: 100%|██████████| 187/187 [00:11<00:00, 15.85it/s]\n",
      "epoch 50 iter 186: train loss 1.03582: 100%|██████████| 187/187 [00:12<00:00, 14.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Your training loop\n",
    "## -- ! code required\n",
    "\n",
    "scheduler = None\n",
    "if config.lr_decay:\n",
    "    warmup_steps = int(config.warmup_tokens // config.batch_size)\n",
    "    decay_steps = int(config.final_tokens // config.batch_size)\n",
    "    lr_lambda = lambda step: min((step + 1) / warmup_steps, 1.0) * (1.0 - max(0, step - warmup_steps) / decay_steps)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(config.max_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader)) \n",
    "    for iter, (x, y) in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x)\n",
    "        loss = F.cross_entropy(loss.view(-1, loss.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}\")\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        epoch_loss = epoch_loss+loss.item()\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "    \n",
    "\n",
    "        \n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "    'loss': best_loss,\n",
    "}, f\"pretrained.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QkMyMkYd61t"
   },
   "source": [
    "Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAIqnLpqd5Xb"
   },
   "source": [
    "A training loss plateau is a common issue when training neural networks. It occurs when the loss stops decreasing and starts oscillating around a certain value, indicating that the model is no longer improving its performance. To overcome this, we can: Adjust the learning rate, Use a different optimizer, Early stopping and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX7m1LVg94Hm"
   },
   "source": [
    "### Q4.2 Load pretrain model and do a prompt writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTQ2ThBSA-u3"
   },
   "source": [
    "Load the best model from Q4.1, provide a prompt and let the model continue writing for you. Feel free to try different prompts or different models.  \n",
    "If your best model is trained on SCC or the other machines, load the model in the jupyter notebook you are going to submit and print out the 'writing'. Screenshot of the output 'writing' will not be accepted. \n",
    "\n",
    "In this question, you are required to \n",
    "- show the prompt writing output\n",
    "- show some discussion for 4.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcDXLTCk_cfZ"
   },
   "source": [
    "#### 4.2.1 What would you do to improve the text generation quality(readability, spelling, grammar, logic etc.) of transformer-based language model? If you refer to some papers or posts, remember to cite them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv8_XoP4dOS0"
   },
   "source": [
    "Token-based decoding strategies: Employ decoding strategies like beam search, nucleus sampling, or temperature scaling that can help in generating more coherent and contextually relevant text. Paper: The Curious Case of Neural Text Degeneration\n",
    "\n",
    "Regularization: Use regularization techniques like dropout, weight decay, or layer normalization to improve the generalization of the model, which can lead to better text generation quality.\n",
    "\n",
    "Adversarial training: Incorporate adversarial training techniques, where a discriminator model is trained to distinguish between real and generated text. This approach encourages the generator to produce text that is more realistic and coherent. Paper: Adversarial Training Methods for Semi-Supervised Text Classification\n",
    "\n",
    "Evaluation metrics: Use comprehensive evaluation metrics like BLEU, ROUGE, or human evaluation to assess the quality of generated text, which will help in identifying areas that need improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U67LicYiEs-K",
    "outputId": "36e5013f-c235-48bd-cfb1-fe4261879778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example output \n",
    "# model architecture should be defined before you load the parameters \n",
    "PATH = 'pretrained.pth'\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yq9WkmU-FBOs",
    "outputId": "618eea84-a09d-46dc-f299-1371b2d6f679",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter turned on the TV,  Riddle's diary face in the school\n",
      "armchairs and saw a pair of tea and then put the blood as though they\n",
      "were about to crash it to their window.\n",
      "\n",
      "\"How did you know who they got to get any more than any\n"
     ]
    }
   ],
   "source": [
    "# Example output \n",
    "prompt =\"Harry Potter turned on the TV,  \"\n",
    "context=[ord(c) for c in prompt]\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(device)\n",
    "y = sample(model, x, 200, temperature=0.6, sample=True, top_k=5)[0]\n",
    "completion = ''.join([chr(train_dataset.itos[int(i)]) for i in y])\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
